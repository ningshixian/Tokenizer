用HMM做中文分词一：序
http://www.52nlp.cn/itenyh%E7%89%88-%E7%94%A8hmm%E5%81%9A%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%9B%9B%EF%BC%9Aa-pure-hmm-%E5%88%86%E8%AF%8D%E5%99%A8

如何进行分词呢？主流的方法有三种：
第1类是基于语言学知识的规则方法，如：各种形态的最大匹配、最少切分方法；
第2类是基于大规模语料库的机器学习方法，这是目前应用比较广泛、效果较好的解决方案．
用到的统计模型有N元语言模型、信道---噪声模型、最大期望、HMM等。
第3类也是实际的分词系统中用到的，即规则与统计等多类方法的综合。

我们假设第i个分类标记只依赖于第i-1个标记，那么我们可以得到分类标记的马尔科夫链了：
P(C1，C2，C3….Ci) =P(C1)P(C2|C1)P(C3|C2)…P(Ci|Ci-1)
我们称P(Cj|Ci)为i到j的状态转移概率，可以通过在大量预料中使用统计的方法求得

Viterbi算法:去掉不必要的计算，要找到最大的隐藏序列，如果在子序列上都不能取得最大还有必要去计算全序列吗？

我们打算构建一个2-gram(bigram)语言模型，也即一个1阶HMM，
每个字符的标签分类只受前一个字符分类的影响。
现在，我们需要求得HMM的状态转移矩阵 A 以及混合矩阵 B。其中：
Aij = P(Cj|Ci)  =  P(Ci,Cj) / P(Ci) = Count(Ci,Cj) / Count(Ci)
Bij = P(Oj|Ci)  =  P(Oj,Ci) / P(Ci) = Count(Oj,Ci) / Count(Ci)
公式中C = {B,E,M,S}，O = {字符集合}，Count代表频率。
在计算Bij时，由于数据的稀疏性，很多字符未出现在训练集中，这导致概率为0的结果出现在B中，
为了修补这个问题，我们采用加1的数据平滑技术，即：
Bij = P(Oj|Ci)  =  (Count(Oj,Ci) + 1)/ Count(Ci)

求得的矩阵A如下：

           B                   M                    E                      S

B  0.0                | 0.19922840916814916 | 0.8007715908318509 | 0.0

M  0.0                | 0.47583202978061256 | 0.5241679702193874 | 0.0

E  0.6309567616935934 | 0.0                 | 0.0                | 0.36904323830640656

S  0.6343402140354506 | 0.0                 | 0.0                | 0.36565844303914763


我们设定初始向量Pi = {0.5, 0.0, 0.0, 0.5}（M和E不可能出现在句子的首位）。
至此，HMM模型构建完毕。其实用统计方法构建HMM并不复杂，麻烦的是写程序，
特别是需要将分类标签和字符集合进行编码，这是个极其繁琐的过程。
